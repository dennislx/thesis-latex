# HiTANet

纵向 EHR 数据中，每个患者数据都可以被视为时间顺序的访问序列，并且在每次访问中，有几个问诊编码. 
- 令 C = {c1,c2, · · · ,cN } 表示所有唯一的诊断代码, c* 抽象地表示整个患者数据，它附加在每个患者数据的末尾? 什么意思
- 令 X = [x1, x2, · · · , xT , x∗] 表示患者的就诊信息，其中第 t 次就诊 xt ∈ {0, 1}^{N+1}, 是一个二元向量, 如果第 i 个诊断代码 ci 在第 t 次访问中出现，则 x_ti = 1. 对于所有患者数据，它们都具有相同的 x*? 甚意思
- dt 表示访问 xt 的相应时间信息, 令 δt = dT − dt 表示最后一次访问和第 t 次访问之间的间隔 (天为单位)

问题描述: 给定患者就诊数据 X = [x1, x2, · · · , xT , x∗](最后一个是虚拟事件), 和时间向量 ∆ = [δ1, δ2, · · · , δT , δ∗](recency?), 风险预测任务的目标是预测患者未来是否会患上目标疾病. 接下来介绍我们是如何解决这个问题的:
1. 对于第 t 次访问，HiTANet 通过考虑相应的诊断代码 xt 和时间间隔 δt 来学习向量表示 vt
2. 然后通过使用基本的 Transformer, HiTANet 可以学习基于 [v1, v2, · · · , vT , v*] 的隐藏状态 ht
3. 接着用隐藏状态去学习除了最后一个虚拟事件之外的局部注意力αt (不带时间信息)
4. 为了对疾病演变(disease progression)进行建模, 必须考虑不规律的时间差信息, 我们将 h* (x*最后一个虚拟事件的抽象表征) 和 时间差表征, 给融合成一个全局级别的注意力 βt
5. 为了获得每一个事件的总体注意力得分 γt, 将注意力得分 α 和 β  以及抽象表示 h∗ 都考虑在内, 根据学习到的总体注意力得分 γ ，HiTANet 生成最终表示 h' 以用于预测

Local Level: Visit Analysis
给定一个稀疏二元访问向量 xt, 首先使用线性函数将其编码到一个相对密集的空间 et ∈ R^m, 如下所示:
因此，每个患者的数据可以表示为 E = [e1, e2, · · · , eT , e*], 虽然大多数最先进的风险预测模型都是建立在以 E 为输入的 RNN 上并且可以取得良好性能的，但是不同访问之间的交互都是在一个黑盒子. 为了明确地模拟这些交互，我们建议使用 Transformer 结构. Transformer允许每个事件都使用自注意力与其他事件进行交互, 这与RNN相比, 大大减少了重要信息的衰减; 如果没有建模时间信息，现有模型很难捕捉到疾病随时间变化的进展。因此，在对erisk数据建模时考虑时间信息是必不可少的

time-lstm是一种基于RNN考虑时间信息重要性的模型, 这类模型要求我们去定义一个stationary decay function, 作用于过去信息的衰减, 如果两个事件相隔太久远, 但抑郁诊断信息又比较相似, 这种情况下t-lstm会过度的衰减过去得到的信息, 然而这类信息其实是对我们病情预测的强化而不应该让他衰减掉. 为了解决这些问题，我们提出了一种新颖的时间感知 Transformer，它首先嵌入时间信息，然后将时间向量作为输入的一部分. 具体来说，我们将时间向量 Δ 和访问向量 E 结合起来作为 Transformer的输入, 但是，Δ 和 E 不在同一个潜在空间中。因此，我们需要将时间向量 Δ 嵌入到潜在访问空间中.

风险预测任务的一个常见假设是最近的访问越多，越重要. 因此，应激活接近最后一次的访问。为了实现这个目标，我们使用element-wise squre来进一步侧重最近发生的事件, 只有档w/b都接近0的时候, 相应的位置才会被激活(注意1-tanh), 样，ft的不同位置可以代表对时间距离的不同偏好。使用这种操作的好处是可以防止远离 0 的值（即 δT）的影响 (nice!!!), 使用嵌入的时间向量 rt ∈ R m，我们可以得到设计的时间感知 Transformer 的输入向量，即 vt = et + rt (transformer的输入)

给定输入矩阵 V = [v1, v2, · · · , vT , v∗], 用transformer学习每个事件之间的长期依赖关系, 并强调时间信息: [h1, h2 · · · , hT , h∗] = F ([v1, v2, · · · , vT , v∗]), 通过在transformer中使用自注意力机制聚合所有其他访问信息. 医生在诊断时，不会只关注当前就诊，而是会查阅历史病历, 寻找与目标疾病高度相关的病历. 为了模拟这样的诊断过程, 我们使用基于局部的注意机制计算每次访问的注意分数, 我们使用自注意力机制学习每个事件的注意分数 ηt（h*除外）

以上只是学到了每一个事件的局部上下文权重, 事实上，医生不仅关注个体就诊，还关注疾病进展, 通过分析整体诊断（即 x*）来做出最终判断. 为了模仿这一步，我们提出了一种新颖的时间感知键查询注意机制。由于 h* 由 Eq.(3) 获得了关于整个病历史的抽象表征, 先用一个relu保留正数, 

在分析整体诊断时，医生还想知道哪些时间点对疾病至关重要. 为此，我们将每个时间信息 δt 嵌入到潜在空间中，如下所示: 一个关于时间嵌入的方程; 你可能会觉得公式2和公式6很相似, 公式2是为了获得transformer的输入, 将时间和文本信息一起嵌入, 着重捕捉与时间信息相关联的诊断代码出现的重要性. 而这里则试图在不考虑任何诊断代码的前提下表征时间信息本身在疾病进展过程中的重要性. 

Following the key-query attention mechanism in Transformer [40], we can obtain an attention weight as follows. 通过分析不同的访问和整体诊断，我们得到两个注意力向量：关注每个访问表示的局部注意力向量α和关注每个时间表示的全局注意力向量β. 为了捕捉不同情况下访问表示和时间表示的偏好，我们提出了一种动态注意力融合机制。特别地，我们首先将整体表示 h* 嵌入到一个新空间中，然后使用 softmax 层对其进行归一化. 然后，我们根据注意力权重和嵌入的整体表示 z 为每次访问生成整体注意力权重. 根据生成的注意力权重和每次访问的隐藏状态，我们最终可以得到一个患者数据的表示如下



该模型由三个主要部分组成。第一个组成部分是本地级别的访问分析
- transformer 用于对每次 EHR 访问建模并生成每次访问的隐藏状态, 然后用于产生局部注意力权重
- 第二个关键组成部分是全球层面的综合分析, 整体诊断表征被用作查询向量，嵌入的时间信息被认为是关键向量, 这样得到一个关于时间的全局注意力
- 动态注意力融合组件用于组合这两个注意力权重

code2id文件: 类似于token2id的东西, 每个病人最后一个id就是加len(code2id)
seq_dignosis_codes (B,L,Mc): 比如某一位: [[361, 1027], [577], [2], [43, 567, 120, 112, 229, 304], [304], [229, 43, 1377], [8692]]
seq_time_step: 比如某一位: [640, 446, 359, 354, 353, 0, 0]
batch_labels: 比如某一位: 1
batch_mask_code: 当前时间下有多少个code, (B, L, Mc)
batch_mask, batch_mask_final: (B,L), final就是最后一个位置是1, 其余0;  

EncoderNew模块:
- 将时间(天)除以180, 把这个时间给放大到64维, 然后 1-tan(x^2), 然后再放大到256维
- 同样把diagnosis-code像word-embedding那样给投射到256维(然后把Mc那个地方给加起来, 为什么不取最大?因为这里同一个时间可以有多个code,所以加起来合适), 另外加上一个可学习的bias项
- 把code-embed和time-feature融合, 然后加上positional embedding, 这里预设了一个某人最长的病历长度为51, 同样融合的办法就是相加
- 然后是不带任何padding的全注意力抽象表征, 然后得到最后一个token(最后一个虚拟事件, 用8692, 即token2id字典的长度)的表征 - 这样得到了`final_statues`(B, 256), 然后 relu(f(x))降维到64
- 以上这几步我们得到transformer-encoded features(B,L,256), 以及queres(B,1,64)代表最后一个虚拟事件
    - self-attention: 将feature的256维降到1维, 然后softmax长度那个维度(不包括最后一个虚拟事件)
    - time-weight: 将时间和最后一个虚拟事件的表征融合, 然后softmax(不包括最后一个虚拟事件)
    - attention-weight: 生成这两种注意力的一个平衡项, 这样保证每一个人的这两项都存在不同权重
- 最后我们将时间-事件编码给加权平均(长度那个维度), dropout, 接classification-layer, 输出两个值, 然后用focal-loss来缩小label和pred之间的差距


# A Time-Aware Transformer Based Model for Suicide Ideation Detection on Social Media

在这项工作中，我们专注于使用用户级时间上下文识别推文中自杀意念的存在. 我们将要评估自杀风险的推文表示为 ti ∈ T = {t1, t2, ··· , tN } 由用户 uj ∈ U = {u1, u2, ··· , uM} 在时间 τ  创作当前. 每条风险评估的推文都伴随这一连串之前的发推历史: H_{i,j} = [(h_1^i, τ_1^i),(h_2^i, τ_2^i), ··· ,(h_L^i, τ_L^i)], 其中h_k^i是用户u_j在时间τ_k^i发布的推文. 

Encoding the Tweet to be Assessed
研究表明，社交媒体用户的语言风格有助于了解他们的心理状态. 我们用预训练的sentence-bert全面地表示推文中的语言特征: (我们发现sentenceBERT表现优于FastText, ELMo等嵌入): T_i = SentenceBert(t_i)

User Historical Emotion Spectrum
个人历史推文编码：情绪反应（Tarrier 等人，2007 年）、强度（Links 等人，2008 年）和不稳定（Palmier-Claus 等人，2012 年）等情绪因素的放大会增加自杀风险. 一般的语言模型文本编码器无法捕获社交媒体帖子中表达的细粒度情感. 为了捕捉细粒度的情绪，我们使用 Plutchik 的情绪轮 (Plutchik, 1980)。这种分类提出了三组八种情绪，排列成四对对立的二元性。轮子描述的主要情绪是：喜悦 - 悲伤、惊讶 - 期待、愤怒 - 恐惧和信任 - 厌恶. 此转换器对每个历史帖子进行标记，并在每个帖子的开头添加 [CLS] 标记。我们使用与此 [CLS] 标记（768 维编码）对应的最终隐藏状态作为情感谱的聚合表示. 我们将每条历史推文$h_k^i$用PlutchikTransformer给编码成一个情绪向量: E_k^i = PlutchikTransformer(h_k^i)

Modeling Historical Tweets Sequentially
历史推文的发布时间间隔可能相差很大，从几秒到几年不等(Wojcik 和 Hughes，2019 年）. 这种变化可能是分析用户随时间变化的情绪状态的重要因素(Sueki, 2015). LSTM 单元假设输入是等距序列，因此无法模拟历史推文发布时间的不规则性. 使用用户历史推文之间的这种相对时间差可以随着时间的推移更准确地逐步模拟用户的情绪. 因此，我们建议使用时间感知 LSTM (T-LSTM)（Baytas 等人，2017 年），其中连续推文之间的时间间隔被馈送到 TLSTM 单元. 因此，T-LSTM 单元结合了推文之间的实际时间差异，以及每条历史推文的情感背景E_i^k. T-LSTM 根据连续元素之间经过的时间对记忆应用时间衰减，并对短期记忆单元 C_k^s 进行加权。直觉上，两条推文之间间隔时间越长, 对彼此的影响就应该越小. 为实现这一点，T-LSTM 使用经过时间的单调递减函数，将时间转换为适当的权重。将时间差融入到Tlstm-cell中: 短期记忆->短期记忆折现->长期记忆->调整后的记忆

其中 C_{k-1} 和 C_k 是上一个和当前的记忆. Δk 是历史推文 h_{k-1} 和 h_k 之间经过的时间. g(·) 是一种启发式衰减函数，它随着 k 的增加而降低短期记忆的影响. 我们根据经验和 Baytas 等人的建议选择 g(k)=1/k. t-lstm在计算当前隐藏状态的时候重新计算上一个时间戳的cell-state!! For each
historic tweet h_i^k, the T-LSTM cell modifies LSTM gate operations to compute the current hidden state (H_k^i) by feeding C_{k-1}^* instead of C_{k-1}.

为了识别推文中是否存在自杀意图，STATENet 以时间感知的方式共同学习要评估的推文语言和情绪历史谱. 此，我们concat单个历史推文表征和整体的隐藏状态 H_k^i，然后是带有线性修正单元 (ReLU) 的密集层 (Hahnloser et al., 2000) 以形成预测向量。最后，使用 softmax 函数 (Goodfellow et al., 2016) 输出存在自杀意图的概率. 表示自杀意图的推文只占数据的一小部分. 为了解决这个类不平衡问题（实际上，现实世界中的不平衡要大得多），我们使用 Cui 等人提出的类平衡损失来训练 STATENet. 该损失函数通过引入与样本数量成反比的加权因子来应用按类重新加权方案


# Time-Aware Transformer-based Network for Clinical Notes Series Prediction

对于由 K 名患者组成的患者队列，与第 k 名患者相关的临床记录序列可以表示为 N^k = {N_1^k, ...N_m^k}. 创建时间为 τ^k = {t_1^k, ..., t_m^k}. m^k代表第$k$个病人总的病历个数. 对于 k, l ∈ [1, K]，m(k) 不一定等于 m(l)(就是说不同人的病历数目不同). 每个病历$N^k_i 包含一系列token {w^k_1 , ..., w^k_n }. 每个病历中标记个数n^k都不一定相同. 我们旨在建立一个模型，该模型使用患者的临床病历序列和相应的创建时间序列,  以及患者临床结果(0表示死亡, 1表示存活), 以预测患者具有阳性临床结果（例如，院内死亡）的概率. 对于患者 k，目标是学习映射： 正如我们在本段开头所讨论的，N(k) 是一个有序的病历序列, 每一个病历也是一个有序的标记序列. 为简单起见，在以下部分中，我们描述了针对单个患者的方法，并在后面删除了上标 (k)。

FTL-Trans Overview

我们提出了一种分层模型结构来捕获临床笔记中的时间和多级顺序信息. 我们提出的模型从临时患者笔记中提取单个患者级别的表示, 可用于预测临床结果的下游任务. 图 2 显示了所提出的模型框架的架构，我们称之为 FTL-Trans: 
- 第一层是块内容嵌入层（第 4.3 节），它读取患者临床记录, 并表征成一个语义向量
- 第二层是 Position-Enhanced Chunk Embedding Layer（第 4.4 节），旨在将语义向量与多级位置信息结合? 怎么结合? 我们的模型就能够解释notes(病历)的顺序信息。
- 第三层是时间感知（time-aware layer）, 将第二层的position-enhanced chunk embedding和time information结合来为下游任务学习一个病历序列的表征. 
- 最后一层则是学习用户级别的表征: 以预测最终的结果. 我们一层一层的阐述细节

块内容嵌入(encode the text within each chunk). 为了对每个块的文本内容进行建模, 我们使用了 ClinicalBERT（Huang 等人，2019 年），这是一个使用医学语料库的预训练 BERT 模型. 由于 BERT 架构对输入序列有最大长度要求，每个患者的笔记 N_i 首先被分成 o_i 块序列 {N_iC1, N_iC2, ..., N_iCo_i}, 而每一个块chunk由p个token组成, NiCj = {w1, ... wp}. 按照基于 BERT 的架构中的常见做法，块的第一个标记是特殊标记“[CLS]”. 然后我们得到每一个块的表征:
        E^{Ni}_j = ClinicalBERT(NiCj)
在ClinicalBERT的研究里, 每一个块的表征被直接用于预测结果
        P(label=1 | E_j^{Ni}) = σ(WE_j^{Ni})
我们这篇研究的贡献是: 1) 在位置增强块嵌入层的帮助下区分来自不同病历的块? 原本的bert就可以把? 2) 在时间感知层的帮助下学习时间重要性

Position-Enhanced Chunk Embedding Layer
这一层的输入来自三个部分: the chunk content embedding E_j^Ni, 位置编码G_i(global position embedding), 以及L_j(position embedding of chunk, local position embedding), Gi表示病历Ni在这个人所有病历中的位置, Lj表示第j个chunkNiCj在病历Ni中的位置, 我们首先将E_j^Ni与Gi和Lj合并, 并输入到线性感知层, 后面接layernormalization(ba et al, 2016)和dropout(srivastava et al, 2014). 得到position-enhanced chunk embedding R_j^Ni

灵活的时间感知长短期记忆(flexible time-aware lstm)将chunk representation Ri 和 当前的事件时间间隔 Δti 作为输入, 连续事件之间经过的时间是不规则的, 先前的记忆 Ct−1 被分解为长期记忆和短期记忆，短期记忆通过灵活的衰减函数 g(Δt) 进行折扣

Time-Aware Layer
时间感知层旨在捕获临床病历序列中的时间信息, 以反应时间间隔的重要性, Baytas 等人提出的时间感知模型之一是时间感知 LSTM (T-LSTM)。 (2017)。在这项工作中，我们提出了灵活的 T-LSTM (FT-LSTM)，它是 T-LSTM 模型的扩展。他们的假设是时间影响将始终以固定模式衰减。然而，这种假设在某些情况下可能不成立，尤其是在临床领域. 我们没有使用非递增函数，而是提出了一种灵活且通用的衰减函数. 我们用他来学习一个灵活的衰减函数. 我们使用双向 FT-LSTM 来捕捉两个方向的时间影响 

Classification Layer
我们使用交叉熵损失来训练模型, L = −(y log (ˆy) + (1 − y) log (1 − yˆ)), 最后的分类层用于进行患者级别的预测, 它以 FT-LSTM 的最后隐藏状态 h(最后一个前向单元→h 和后向单元 ←h 的串联作为输入), dropout->linear->sigmoid, 最后输出一个[0,1]之间的预测分数, 使用交叉熵损失来训练我们的模型


# Semantic Similarity Models for Depression Severity Estimation

