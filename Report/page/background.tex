\chapter{Literature Survey}

% \pagebreak

\section{Base Papers}

\subsection{Engineering Base Paper}

\noindent
\textbf{Weighted Word2Vec Based on the Distance of Words}
\textit{\textbf{Authors:} Chia-Yang Chang, Shie-Jue Lee and Chih-Chin Lai}

Word2vec~\citep{chang2017weighted} is a novel technique for the study and application of natural language processing (NLP). It trains a word embedding neural network model with a large training corpus. After the model is trained, each word is represented by a vector in the specified vector space. The vectors obtained possess many interesting and useful characteristics that are implicitly embedded with the original words. The idea of word2vec is that there are relations between the words if they appear in the neighborhood.